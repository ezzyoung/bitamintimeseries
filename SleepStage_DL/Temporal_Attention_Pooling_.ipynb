{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "M3xyEzW7k_Oz"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIdt_KlZ3azT",
        "outputId": "41aa141f-0ead-4341-f2ea-76db6a4c4ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 만들기 끝"
      ],
      "metadata": {
        "id": "FonE8u7w3xgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pkl 파일 로드"
      ],
      "metadata": {
        "id": "AMCa8WtXnq0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = torch.load('/content/drive/MyDrive/Colab Notebooks/sleep_classification_checkpoint.pkl', map_location='cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "WBCLAubTnuG0",
        "outputId": "8f4f0a22-33c8-481f-81a6-3bb093c4207b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/sleep_classification_checkpoint.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a14432283cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# (1) Load with torch, map_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/sleep_classification_checkpoint.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# (2) 데이터 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/sleep_classification_checkpoint.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rfdgwKBnzQ2",
        "outputId": "e9b4a921-457b-4296-bf44-55bce24aa36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 여기서부터 진짜 시작"
      ],
      "metadata": {
        "id": "xZRRyIFV3IoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qp81nU315Zi",
        "outputId": "be1cc28a-b4bd-441b-d163-686de0aa71cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의"
      ],
      "metadata": {
        "id": "cVeA6p-518gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# DSConv Block\n",
        "class DSConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size=3, padding=1, groups=in_ch)\n",
        "        self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm1d(out_ch)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return self.act(self.bn(x))\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_ch=1, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            DSConvBlock(in_ch, base_ch),\n",
        "            DSConvBlock(base_ch, base_ch * 2),\n",
        "            DSConvBlock(base_ch * 2, base_ch * 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)  # (B, C, L)\n",
        "\n",
        "# Lite Transformer Encoder\n",
        "class LiteTransformer(nn.Module):\n",
        "    def __init__(self, d_model=64, heads=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pos_embed = self._positional_encoding(x.size(1), x.size(2), x.device)\n",
        "        x = x + pos_embed\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        return self.norm(x + attn_out)\n",
        "\n",
        "    def _positional_encoding(self, seq_len, dim, device):\n",
        "        pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        i = torch.arange(0, dim, 2, dtype=torch.float32, device=device)\n",
        "        angle_rates = 1 / torch.pow(10000, (i / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        pe = torch.zeros(seq_len, dim, device=device)\n",
        "        pe[:, 0::2] = torch.sin(angle_rads)\n",
        "        pe[:, 1::2] = torch.cos(angle_rads)\n",
        "        return pe.unsqueeze(0)  # (1, seq_len, dim)\n",
        "\n",
        "\n",
        "# Cross-Modal Attention\n",
        "class CrossModalTransformer(nn.Module):\n",
        "    def __init__(self, dim=64, heads=4):\n",
        "        super().__init__()\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, query, context):\n",
        "        out, _ = self.cross_attn(query, context, context)\n",
        "        return self.norm(out + query)\n",
        "\n",
        "# Temporal Attention Pooling 적용\n",
        "class TemporalAttentionPooling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.q = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.attn = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, x):  # x: (B, L, D)\n",
        "        score = torch.softmax(self.attn(x), dim=1)  # (B, L, 1)\n",
        "        pooled = (x * score).sum(dim=1)             # (B, D)\n",
        "        return pooled, score\n",
        "\n",
        "# 전체 모델 구성\n",
        "class CrossModalSleepNet(nn.Module):\n",
        "    def __init__(self, cnn_dim=64, transformer_dim=64, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.eeg_cnn = CNNFeatureExtractor(1, cnn_dim)\n",
        "        self.eog_cnn = CNNFeatureExtractor(1, cnn_dim)\n",
        "\n",
        "        self.project = nn.Linear(cnn_dim * 4, transformer_dim)\n",
        "\n",
        "        self.lite_transformer = LiteTransformer(transformer_dim)\n",
        "        self.cross_transformer = CrossModalTransformer(transformer_dim)\n",
        "        self.temporal_pool = TemporalAttentionPooling(transformer_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(transformer_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: (B, 2, T)\n",
        "        eeg, eog = x[:, 0:1, :], x[:, 1:2, :]\n",
        "        eeg_feat = self.eeg_cnn(eeg)  # (B, C, L)\n",
        "        eog_feat = self.eog_cnn(eog)\n",
        "\n",
        "        eeg_feat = rearrange(eeg_feat, 'b c l -> b l c')\n",
        "        eog_feat = rearrange(eog_feat, 'b c l -> b l c')\n",
        "\n",
        "        eeg_feat = self.project(eeg_feat)\n",
        "        eog_feat = self.project(eog_feat)\n",
        "\n",
        "        eeg_feat = self.lite_transformer(eeg_feat)\n",
        "        fused_feat = self.cross_transformer(eeg_feat, eog_feat)\n",
        "\n",
        "        pooled, attn_weights = self.temporal_pool(fused_feat)\n",
        "        out = self.classifier(pooled)\n",
        "\n",
        "        return out, attn_weights"
      ],
      "metadata": {
        "id": "0Hs3Zrif16Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 합치기"
      ],
      "metadata": {
        "id": "DIE2tGV-vzJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 병합할 pkl 경로 설정\n",
        "pkl_dir = \"/content/drive/MyDrive/sleep_segments\"\n",
        "file_list = [os.path.join(pkl_dir, f\"sleep_data_part{i}.pkl\") for i in range(1, 7)]\n",
        "\n",
        "all_segments, all_labels = [], []\n",
        "\n",
        "print(\"🔄 피클 파일 병합 중...\")\n",
        "for file in file_list:\n",
        "    with open(file, 'rb') as f:\n",
        "        segments, labels = pickle.load(f)\n",
        "        for seg in segments:\n",
        "            tensor_seg = torch.tensor(seg, dtype=torch.float32)\n",
        "            if tensor_seg.ndim == 3:  # (2, 60, 50)\n",
        "                tensor_seg = tensor_seg.view(2, -1)  # (2, 3000)\n",
        "            all_segments.append(tensor_seg)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "all_segments = torch.stack(all_segments)  # (N, 2, T)\n",
        "all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/sleep_segments/merged_sleep_dataset.pt\"\n",
        "torch.save((all_segments, all_labels), save_path)\n",
        "print(f\"✅ 병합 완료! 저장 위치: {save_path}\")"
      ],
      "metadata": {
        "id": "T7_ivoKhCsbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef441f9-52cb-4c82-f9e5-16768bd45d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 피클 파일 병합 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3bdd17a6ae7d>:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  tensor_seg = torch.tensor(seg, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 병합 완료! 저장 위치: /content/drive/MyDrive/sleep_segments/merged_sleep_dataset.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "class SleepTensorDataset(Dataset):\n",
        "    def __init__(self, data_tensor, label_tensor):\n",
        "        self.data = data_tensor\n",
        "        self.labels = label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# 불러오기\n",
        "merged_path = \"/content/drive/MyDrive/sleep_segments/merged_sleep_dataset.pt\"\n",
        "print(\"🔄 병합된 데이터 불러오는 중...\")\n",
        "all_data, all_labels = torch.load(merged_path)\n",
        "print(f\"✅ 불러오기 완료: 총 샘플 = {len(all_data)}\")\n",
        "\n",
        "# Train/Test Split\n",
        "indices = np.arange(len(all_data))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "split = int(len(indices) * 0.8)\n",
        "train_idx, test_idx = indices[:split], indices[split:]\n",
        "\n",
        "train_dataset = SleepTensorDataset(all_data[train_idx], all_labels[train_idx])\n",
        "test_dataset  = SleepTensorDataset(all_data[test_idx], all_labels[test_idx])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"✅ DataLoader 생성 완료: Train = {len(train_dataset)} / Test = {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCBARnMLFl2L",
        "outputId": "c8937f76-06af-47ad-de3c-8c919c5baebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 병합된 데이터 불러오는 중...\n",
            "✅ 불러오기 완료: 총 샘플 = 326557\n",
            "✅ DataLoader 생성 완료: Train = 261245 / Test = 65312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 파이프라인"
      ],
      "metadata": {
        "id": "PRHBfvuQHMpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습/평가 정의"
      ],
      "metadata": {
        "id": "E8JSHb7FMV7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for x, y in tqdm(dataloader, desc=\"Train\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = output.argmax(1)\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    acc = correct / total * 100\n",
        "    return total_loss / len(dataloader), acc"
      ],
      "metadata": {
        "id": "4mtJ94e8M629"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output, _ = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    # 전체 성능\n",
        "    acc = 100. * correct / total\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "\n",
        "    macro_f1 = report['macro avg']['f1-score']\n",
        "    precision = report['macro avg']['precision']\n",
        "    recall = report['macro avg']['recall']\n",
        "\n",
        "    # REM class = 4 기준 성능\n",
        "    rem_acc = report.get(\"4\", {}).get(\"recall\", 0.0)\n",
        "    rem_f1 = report.get(\"4\", {}).get(\"f1-score\", 0.0)\n",
        "\n",
        "    return total_loss / len(dataloader), acc, macro_f1, precision, recall, rem_acc, rem_f1"
      ],
      "metadata": {
        "id": "zeo25sdfNFF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pt 불러오기\n",
        "x_tensor, y_tensor = torch.load('/content/drive/MyDrive/sleep_segments/merged_sleep_dataset.pt')\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "train_len = int(len(dataset) * 0.8)\n",
        "train_set, test_set = random_split(dataset, [train_len, len(dataset) - train_len])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"✅ DataLoader 생성 완료: Train = {len(train_set)}개 / Test = {len(test_set)}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1kh4KOG_bl8",
        "outputId": "5e1a72ee-f168-4d0d-9d53-739d462f4be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DataLoader 생성 완료: Train = 261245개 / Test = 65312개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "u6qYVnm0nR6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Class weights 계산 및 손실 함수 정의\n",
        "y_train = [label.item() for _, label in train_set]\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)"
      ],
      "metadata": {
        "id": "gJlE99Gmo8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델, 손실 함수, 옵티마이저 정의\n",
        "model = CrossModalSleepNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 학습 기록\n",
        "train_losses, val_losses, val_accuracies = [], [], []\n",
        "all_f1, all_precision, all_recall = [], [], []\n",
        "\n",
        "# EarlyStopping 설정\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(1, 21):  # ⏱️ Epoch 20까지\n",
        "    print(f\"\\n🌀 Epoch {epoch}\")\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    val_loss, val_acc, f1, prec, rec, rem_acc, rem_f1 = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | F1: {f1:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f}\")\n",
        "    print(f\"REM Class ▶ Accuracy (Recall): {rem_acc:.3f} | F1 Score: {rem_f1:.3f}\")\n",
        "\n",
        "\n",
        "    # 기록 저장\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    all_f1.append(f1)\n",
        "    all_precision.append(prec)\n",
        "    all_recall.append(rec)\n",
        "\n",
        "    # 🛑 EarlyStopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n⏹️ Early stopping at epoch {epoch}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "# 가장 성능 좋았던 모델 불러오기\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "print(\"\\n📊 최종 모델 평가 요약:\")\n",
        "print(f\"Average Validation Accuracy: {np.mean(val_accuracies):.2f}%\")\n",
        "print(f\"Average Macro F1 Score: {np.mean(all_f1):.4f}\")\n",
        "print(f\"Average Precision: {np.mean(all_precision):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(all_recall):.4f}\")\n",
        "print(f\"Last Epoch Accuracy: {val_accuracies[-1]:.2f}% | F1: {all_f1[-1]:.4f} | Precision: {all_precision[-1]:.4f} | Recall: {all_recall[-1]:.4f}\")\n",
        "print(f\"🔍 REM Class ▶ Accuracy (Recall): {rem_acc:.3f} | F1 Score: {rem_f1:.3f}\")"
      ],
      "metadata": {
        "id": "8y_3FYKc_k40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2600ebe4-246d-4f42-ea57-ff9d29a2f34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🌀 Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:01<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7398 | Train Accuracy: 81.72%\n",
            "Val Loss: 4.0243 | Val Acc: 68.97% | F1: 0.163 | Precision: 0.138 | Recall: 0.200\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6016 | Train Accuracy: 85.82%\n",
            "Val Loss: 3.8612 | Val Acc: 67.93% | F1: 0.163 | Precision: 0.139 | Recall: 0.197\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5672 | Train Accuracy: 86.66%\n",
            "Val Loss: 4.5327 | Val Acc: 68.97% | F1: 0.163 | Precision: 0.138 | Recall: 0.200\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5448 | Train Accuracy: 87.07%\n",
            "Val Loss: 3.2002 | Val Acc: 45.94% | F1: 0.133 | Precision: 0.130 | Recall: 0.143\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5318 | Train Accuracy: 87.41%\n",
            "Val Loss: 3.6661 | Val Acc: 65.04% | F1: 0.162 | Precision: 0.141 | Recall: 0.193\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5182 | Train Accuracy: 87.73%\n",
            "Val Loss: 3.7168 | Val Acc: 49.82% | F1: 0.136 | Precision: 0.126 | Recall: 0.149\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "🌀 Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 16328/16328 [32:01<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5066 | Train Accuracy: 88.09%\n",
            "Val Loss: 3.9360 | Val Acc: 68.90% | F1: 0.164 | Precision: 0.147 | Recall: 0.200\n",
            "REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n",
            "\n",
            "⏹️ Early stopping at epoch 7. Best Val Loss: 3.2002\n",
            "\n",
            "📊 최종 모델 평가 요약:\n",
            "Average Validation Accuracy: 62.23%\n",
            "Average Macro F1 Score: 0.1549\n",
            "Average Precision: 0.1370\n",
            "Average Recall: 0.1833\n",
            "Last Epoch Accuracy: 68.90% | F1: 0.1637 | Precision: 0.1474 | Recall: 0.2000\n",
            "🔍 REM Class ▶ Accuracy (Recall): 0.000 | F1 Score: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🧪 테스트 세트 성능 평가:\")\n",
        "test_loss, test_acc, test_f1, test_prec, test_rec, rem_acc, rem_f1 = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Macro F1: {test_f1:.4f} | Precision: {test_prec:.4f} | Recall: {test_rec:.4f}\")\n",
        "print(f\"REM Accuracy: {rem_acc:.4f} | REM F1: {rem_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_JWVJzFskox",
        "outputId": "871f2d64-bdef-4c3e-e7b8-5d7eb7b3724e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 테스트 세트 성능 평가:\n",
            "Test Loss: 3.9360 | Accuracy: 68.90%\n",
            "Macro F1: 0.1637 | Precision: 0.1474 | Recall: 0.2000\n",
            "REM Accuracy: 0.0000 | REM F1: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# 모델 학습이 끝난 후 (예: best 모델 기준)\n",
        "model_path_pt = '/content/drive/MyDrive/best_model_trans.pt'\n",
        "model_path_pkl = '/content/drive/MyDrive/best_model_trans.pkl'\n",
        "\n",
        "#저장 - PyTorch 공식 방식 (.pt)\n",
        "torch.save(model.state_dict(), model_path_pt)\n",
        "\n",
        "# 저장 - pickle 방식 (.pkl)\n",
        "with open(model_path_pkl, 'wb') as f:\n",
        "    pickle.dump(model.state_dict(), f)\n",
        "\n",
        "print(\"✅ 모델 저장 완료 (.pt, .pkl)\")"
      ],
      "metadata": {
        "id": "M73ODL1l0V5d",
        "outputId": "243ec3c3-50da-4730-fbc9-a7840bb3cea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 모델 저장 완료 (.pt, .pkl)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##평가"
      ],
      "metadata": {
        "id": "hT6axtPawYxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 모드\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        x = x.to(device)\n",
        "        out, _ = model(x)\n",
        "        preds = out.argmax(dim=1).cpu()\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_true.extend(y.tolist())\n",
        "\n",
        "# ✅ 평가 지표 출력\n",
        "print(\"✅ 평가 결과\")\n",
        "print(\"Accuracy      :\", round(accuracy_score(y_true, y_pred) * 100, 2), \"%\")\n",
        "print(\"Macro F1      :\", round(f1_score(y_true, y_pred, average=\"macro\") * 100, 2), \"%\")\n",
        "print(\"Macro Precision:\", round(precision_score(y_true, y_pred, average=\"macro\") * 100, 2), \"%\")\n",
        "print(\"Macro Recall  :\", round(recall_score(y_true, y_pred, average=\"macro\") * 100, 2), \"%\")"
      ],
      "metadata": {
        "id": "h5htiOSXJEKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4165a7-8053-49bd-8e09-a351f1992c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 4082/4082 [00:18<00:00, 223.33it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 평가 결과\n",
            "Accuracy      : 68.8 %\n",
            "Macro F1      : 16.3 %\n",
            "Macro Precision: 13.77 %\n",
            "Macro Recall  : 19.99 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxI5OcgJJGks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}